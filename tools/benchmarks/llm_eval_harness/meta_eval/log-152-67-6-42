Fri Nov 22 23:00:22 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:00:34,997 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:00:34,997 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:00:46,485 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:00:46,486 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:00:46,493 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:00:46,493 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.9, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:00:58 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
INFO 11-22 23:00:58 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:00:59 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:00:59 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:01:00 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:01:01 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.90s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.90s/it]

INFO 11-22 23:01:03 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:01:03 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=31.41GiB gpu_memory_utilization=0.90
INFO 11-22 23:01:03 gpu_executor.py:113] # GPU blocks: 64324, # CPU blocks: 8192
INFO 11-22 23:01:03 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 125.63x
INFO 11-22 23:01:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:01:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:01:18 model_runner.py:1518] Graph capturing finished in 12 secs, took 0.14 GiB
2024-11-22:23:01:23,721 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:01:23,733 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:01:25,353 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:01:25,356 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 28403.86it/s]
2024-11-22:23:01:25,369 INFO     [evaluator.py:438] Running generate_until requests
Running generate_until requests:   0%|          | 0/3 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][AProcessed prompts: 100%|██████████| 3/3 [00:00<00:00, 96.73it/s, est. speed input: 49325.30 toks/s, output: 97.07 toks/s]
Running generate_until requests: 100%|██████████| 3/3 [00:00<00:00, 91.29it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/bin/lm_eval", line 8, in <module>
[rank0]:     sys.exit(cli_evaluate())
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/__main__.py", line 375, in cli_evaluate
[rank0]:     results = evaluator.simple_evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 277, in simple_evaluate
[rank0]:     results = evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 548, in evaluate
[rank0]:     task_output.calculate_aggregate_metric(bootstrap_iters=bootstrap_iters)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator_utils.py", line 98, in calculate_aggregate_metric
[rank0]:     self.agg_metrics[metric_key] = agg_fn(items)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/api/metrics.py", line 27, in mean
[rank0]:     return sum(arr) / len(arr)
[rank0]: TypeError: unsupported operand type(s) for +: 'int' and 'list'
[rank0]:[W1122 23:01:26.265258731 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:01:28 UTC 2024
Fri Nov 22 23:10:46 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:10:58,856 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:10:58,856 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:11:10,588 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:11:10,589 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:11:10,596 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:11:10,596 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.9, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:11:22 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 11-22 23:11:22 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:11:23 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:11:23 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:11:24 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:11:24 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.91s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.91s/it]

INFO 11-22 23:11:27 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:11:27 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=31.41GiB gpu_memory_utilization=0.90
INFO 11-22 23:11:27 gpu_executor.py:113] # GPU blocks: 64324, # CPU blocks: 8192
INFO 11-22 23:11:27 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 125.63x
INFO 11-22 23:11:30 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:11:30 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:11:42 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-22:23:11:47,858 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:11:47,871 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:11:49,458 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:11:49,461 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 27776.85it/s]
2024-11-22:23:11:49,474 INFO     [evaluator.py:438] Running generate_until requests
Running generate_until requests:   0%|          | 0/3 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:  33%|███▎      | 1/3 [00:02<00:04,  2.15s/it, est. speed input: 245.10 toks/s, output: 237.66 toks/s][AProcessed prompts: 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, est. speed input: 705.79 toks/s, output: 712.73 toks/s]
Running generate_until requests:  33%|███▎      | 1/3 [00:02<00:04,  2.16s/it]Running generate_until requests: 100%|██████████| 3/3 [00:02<00:00,  1.39it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/bin/lm_eval", line 8, in <module>
[rank0]:     sys.exit(cli_evaluate())
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/__main__.py", line 375, in cli_evaluate
[rank0]:     results = evaluator.simple_evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 277, in simple_evaluate
[rank0]:     results = evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 548, in evaluate
[rank0]:     task_output.calculate_aggregate_metric(bootstrap_iters=bootstrap_iters)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator_utils.py", line 98, in calculate_aggregate_metric
[rank0]:     self.agg_metrics[metric_key] = agg_fn(items)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/api/metrics.py", line 27, in mean
[rank0]:     return sum(arr) / len(arr)
[rank0]: TypeError: unsupported operand type(s) for +: 'int' and 'list'
[rank0]:[W1122 23:11:52.482079246 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:11:54 UTC 2024
Fri Nov 22 23:13:13 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:13:26,075 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:13:26,075 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:13:37,219 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:13:37,219 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:13:37,225 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:13:37,225 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.9, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:13:47 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
INFO 11-22 23:13:47 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:13:49 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:13:49 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:13:50 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:13:50 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.91s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.91s/it]

INFO 11-22 23:13:52 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:13:53 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=31.41GiB gpu_memory_utilization=0.90
INFO 11-22 23:13:53 gpu_executor.py:113] # GPU blocks: 64324, # CPU blocks: 8192
INFO 11-22 23:13:53 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 125.63x
INFO 11-22 23:13:56 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:13:56 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:14:08 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-22:23:14:13,470 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:14:13,483 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:14:15,083 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:14:15,087 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 23741.34it/s]
2024-11-22:23:14:15,100 INFO     [evaluator.py:438] Running generate_until requests
Running generate_until requests:   0%|          | 0/3 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:  33%|███▎      | 1/3 [00:02<00:04,  2.16s/it, est. speed input: 244.71 toks/s, output: 237.28 toks/s][AProcessed prompts: 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, est. speed input: 704.66 toks/s, output: 711.59 toks/s]
Running generate_until requests:  33%|███▎      | 1/3 [00:02<00:04,  2.16s/it]Running generate_until requests: 100%|██████████| 3/3 [00:02<00:00,  1.39it/s]
2024-11-22:23:14:19,568 INFO     [evaluation_tracker.py:182] Saving results aggregated
2024-11-22:23:14:19,575 INFO     [evaluation_tracker.py:258] Saving per-sample results for: meta_mmlu
vllm (pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42), gen_kwargs: (None), limit: 3.0, num_fewshot: None, batch_size: auto
|    Tasks    |Version|Filter|n-shot|  Metric   |   |Value|   |Stderr|
|-------------|-------|------|-----:|-----------|---|----:|---|-----:|
| - meta_mmlu |      1|none  |     0|exact_match|↑  |    0|±  |     0|
|meta_pretrain|N/A    |none  |     0|exact_match|↑  |    0|±  |     0|

|   Groups    |Version|Filter|n-shot|  Metric   |   |Value|   |Stderr|
|-------------|-------|------|-----:|-----------|---|----:|---|-----:|
|meta_pretrain|N/A    |none  |     0|exact_match|↑  |    0|±  |     0|

[rank0]:[W1122 23:14:20.503752024 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:14:22 UTC 2024
Fri Nov 22 23:24:56 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:25:09,286 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:25:09,286 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:25:20,908 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:25:20,909 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:25:20,916 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:25:20,916 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.9, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:25:31 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
INFO 11-22 23:25:31 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:25:33 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:25:33 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:25:34 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:25:34 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.90s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.90s/it]

INFO 11-22 23:25:36 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:25:37 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=31.41GiB gpu_memory_utilization=0.90
INFO 11-22 23:25:37 gpu_executor.py:113] # GPU blocks: 64324, # CPU blocks: 8192
INFO 11-22 23:25:37 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 125.63x
INFO 11-22 23:25:40 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:25:40 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:25:52 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-22:23:25:57,244 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:25:57,257 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:25:58,863 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:25:58,866 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 28339.89it/s]
2024-11-22:23:25:58,878 INFO     [evaluator.py:438] Running generate_until requests
Running generate_until requests:   0%|          | 0/3 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:  33%|███▎      | 1/3 [00:02<00:04,  2.15s/it, est. speed input: 245.38 toks/s, output: 237.93 toks/s][AProcessed prompts: 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, est. speed input: 706.58 toks/s, output: 713.53 toks/s]
Running generate_until requests:  33%|███▎      | 1/3 [00:02<00:04,  2.15s/it]Running generate_until requests: 100%|██████████| 3/3 [00:02<00:00,  1.39it/s]
2024-11-22:23:26:03,361 INFO     [evaluation_tracker.py:182] Saving results aggregated
2024-11-22:23:26:03,367 INFO     [evaluation_tracker.py:258] Saving per-sample results for: meta_mmlu
vllm (pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42), gen_kwargs: (None), limit: 3.0, num_fewshot: None, batch_size: auto
|    Tasks    |Version|Filter|n-shot|  Metric   |   |Value|   |Stderr|
|-------------|-------|------|-----:|-----------|---|----:|---|-----:|
| - meta_mmlu |      1|none  |     0|exact_match|↑  |    0|±  |     0|
|meta_pretrain|N/A    |none  |     0|exact_match|↑  |    0|±  |     0|

|   Groups    |Version|Filter|n-shot|  Metric   |   |Value|   |Stderr|
|-------------|-------|------|-----:|-----------|---|----:|---|-----:|
|meta_pretrain|N/A    |none  |     0|exact_match|↑  |    0|±  |     0|

[rank0]:[W1122 23:26:04.318719284 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:26:06 UTC 2024
Fri Nov 22 23:27:37 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:27:50,616 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:27:50,617 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:28:02,194 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:28:02,195 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:28:02,202 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:28:02,202 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.9, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:28:13 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
INFO 11-22 23:28:13 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:28:14 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:28:15 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:28:15 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:28:16 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.87s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.87s/it]

INFO 11-22 23:28:18 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:28:18 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=31.41GiB gpu_memory_utilization=0.90
INFO 11-22 23:28:19 gpu_executor.py:113] # GPU blocks: 64324, # CPU blocks: 8192
INFO 11-22 23:28:19 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 125.63x
INFO 11-22 23:28:22 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:28:22 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:28:33 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-22:23:28:35,740 WARNING  [task.py:102] [meta_mmlu] passed `generation_kwargs`, but not using `output_type: generate_until`!
2024-11-22:23:28:39,340 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:28:39,353 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:28:40,955 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:28:40,958 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]2024-11-22:23:28:40,971 ERROR    [task.py:1244] doc_to_choice was called but not set in config
  0%|          | 0/3 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/bin/lm_eval", line 8, in <module>
[rank0]:     sys.exit(cli_evaluate())
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/__main__.py", line 375, in cli_evaluate
[rank0]:     results = evaluator.simple_evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 277, in simple_evaluate
[rank0]:     results = evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 393, in evaluate
[rank0]:     task.build_all_requests(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/api/task.py", line 444, in build_all_requests
[rank0]:     inst = self.construct_requests(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/api/task.py", line 1272, in construct_requests
[rank0]:     choices = self.doc_to_choice(doc)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/api/task.py", line 1248, in doc_to_choice
[rank0]:     if isinstance(doc_to_choice, str):
[rank0]: UnboundLocalError: local variable 'doc_to_choice' referenced before assignment
[rank0]:[W1122 23:28:41.800823771 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:28:44 UTC 2024
Fri Nov 22 23:31:00 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:31:12,238 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:31:12,238 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:31:23,633 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:31:23,634 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:31:23,640 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:31:23,640 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.9, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:31:34 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
INFO 11-22 23:31:34 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:31:35 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:31:36 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:31:36 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:31:37 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.91s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.91s/it]

INFO 11-22 23:31:39 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:31:39 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=31.41GiB gpu_memory_utilization=0.90
INFO 11-22 23:31:39 gpu_executor.py:113] # GPU blocks: 64324, # CPU blocks: 8192
INFO 11-22 23:31:39 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 125.63x
INFO 11-22 23:31:43 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:31:43 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:31:54 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-22:23:31:56,614 WARNING  [task.py:102] [meta_mmlu] passed `generation_kwargs`, but not using `output_type: generate_until`!
2024-11-22:23:31:59,876 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:31:59,888 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:32:01,485 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:32:01,488 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 28662.67it/s]
2024-11-22:23:32:01,501 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][AINFO 11-22 23:32:01 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20241122-233201.pkl...
INFO 11-22 23:32:01 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20241122-233201.pkl.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 116, in _wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1697, in execute_model
[rank0]:     output: SamplerOutput = self.model.sample(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 577, in sample
[rank0]:     next_tokens = self.sampler(logits, sampling_metadata)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 281, in forward
[rank0]:     probs = torch.softmax(logits, dim=-1, dtype=torch.float)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.91 GiB. GPU 0 has a total capacity of 39.38 GiB of which 569.38 MiB is free. Including non-PyTorch memory, this process has 38.81 GiB memory in use. Of the allocated memory 38.16 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 54.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/bin/lm_eval", line 8, in <module>
[rank0]:     sys.exit(cli_evaluate())
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/__main__.py", line 375, in cli_evaluate
[rank0]:     results = evaluator.simple_evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 277, in simple_evaluate
[rank0]:     results = evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 449, in evaluate
[rank0]:     resps = getattr(lm, reqtype)(cloned_reqs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/api/model.py", line 371, in loglikelihood
[rank0]:     return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 443, in _loglikelihood_tokens
[rank0]:     outputs = self._model_generate(requests=inputs, generate=False)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 274, in _model_generate
[rank0]:     outputs = self.model.generate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/utils.py", line 1063, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 406, in generate
[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 942, in _run_engine
[rank0]:     step_outputs = self.llm_engine.step()
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 1454, in step
[rank0]:     outputs = self.model_executor.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 125, in execute_model
[rank0]:     output = self.driver_worker.execute_model(execute_model_req)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 343, in execute_model
[rank0]:     output = self.model_runner.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 152, in _wrapper
[rank0]:     raise type(err)(
[rank0]: torch.OutOfMemoryError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241122-233201.pkl): CUDA out of memory. Tried to allocate 2.91 GiB. GPU 0 has a total capacity of 39.38 GiB of which 569.38 MiB is free. Including non-PyTorch memory, this process has 38.81 GiB memory in use. Of the allocated memory 38.16 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 54.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
[rank0]:[W1122 23:32:02.583684941 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:32:04 UTC 2024
Fri Nov 22 23:32:43 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:32:56,210 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:32:56,210 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:33:07,327 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:33:07,328 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:33:07,335 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:33:07,335 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.9, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:33:17 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
INFO 11-22 23:33:17 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:33:19 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:33:19 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:33:20 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:33:20 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.89s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.89s/it]

INFO 11-22 23:33:22 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:33:23 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=31.41GiB gpu_memory_utilization=0.90
INFO 11-22 23:33:23 gpu_executor.py:113] # GPU blocks: 64324, # CPU blocks: 8192
INFO 11-22 23:33:23 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 125.63x
INFO 11-22 23:33:26 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:33:26 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:33:38 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-22:23:33:40,172 WARNING  [task.py:102] [meta_mmlu] passed `generation_kwargs`, but not using `output_type: generate_until`!
2024-11-22:23:33:43,326 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:33:43,338 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:33:44,934 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:33:44,937 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 27654.75it/s]
2024-11-22:23:33:44,950 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][AINFO 11-22 23:33:45 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20241122-233345.pkl...
INFO 11-22 23:33:45 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20241122-233345.pkl.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 116, in _wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1697, in execute_model
[rank0]:     output: SamplerOutput = self.model.sample(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 577, in sample
[rank0]:     next_tokens = self.sampler(logits, sampling_metadata)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 281, in forward
[rank0]:     probs = torch.softmax(logits, dim=-1, dtype=torch.float)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.91 GiB. GPU 0 has a total capacity of 39.38 GiB of which 569.38 MiB is free. Including non-PyTorch memory, this process has 38.81 GiB memory in use. Of the allocated memory 38.16 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 54.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/bin/lm_eval", line 8, in <module>
[rank0]:     sys.exit(cli_evaluate())
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/__main__.py", line 375, in cli_evaluate
[rank0]:     results = evaluator.simple_evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 277, in simple_evaluate
[rank0]:     results = evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 449, in evaluate
[rank0]:     resps = getattr(lm, reqtype)(cloned_reqs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/api/model.py", line 371, in loglikelihood
[rank0]:     return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 443, in _loglikelihood_tokens
[rank0]:     outputs = self._model_generate(requests=inputs, generate=False)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 274, in _model_generate
[rank0]:     outputs = self.model.generate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/utils.py", line 1063, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 406, in generate
[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 942, in _run_engine
[rank0]:     step_outputs = self.llm_engine.step()
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 1454, in step
[rank0]:     outputs = self.model_executor.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 125, in execute_model
[rank0]:     output = self.driver_worker.execute_model(execute_model_req)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 343, in execute_model
[rank0]:     output = self.model_runner.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 152, in _wrapper
[rank0]:     raise type(err)(
[rank0]: torch.OutOfMemoryError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241122-233345.pkl): CUDA out of memory. Tried to allocate 2.91 GiB. GPU 0 has a total capacity of 39.38 GiB of which 569.38 MiB is free. Including non-PyTorch memory, this process has 38.81 GiB memory in use. Of the allocated memory 38.16 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 54.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
[rank0]:[W1122 23:33:45.970103494 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:33:48 UTC 2024
Fri Nov 22 23:35:28 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.7,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
bash: line 1: lm_eval: command not found
end: Fri Nov 22 23:35:28 UTC 2024
Fri Nov 22 23:35:41 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.7,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:35:54,287 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:35:54,287 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:36:05,620 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:36:05,620 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:36:05,626 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:36:05,627 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.7, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:36:16 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 11-22 23:36:16 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:36:18 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:36:18 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:36:19 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:36:19 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.92s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.92s/it]

INFO 11-22 23:36:21 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:36:22 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=23.53GiB gpu_memory_utilization=0.70
INFO 11-22 23:36:22 gpu_executor.py:113] # GPU blocks: 48194, # CPU blocks: 8192
INFO 11-22 23:36:22 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 94.13x
INFO 11-22 23:36:25 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:36:25 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:36:37 model_runner.py:1518] Graph capturing finished in 12 secs, took 0.14 GiB
2024-11-22:23:36:39,020 WARNING  [task.py:102] [meta_mmlu] passed `generation_kwargs`, but not using `output_type: generate_until`!
2024-11-22:23:36:42,340 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:36:42,353 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:36:43,953 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:36:43,956 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 23301.69it/s]
2024-11-22:23:36:43,970 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][AINFO 11-22 23:36:44 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20241122-233644.pkl...
INFO 11-22 23:36:44 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20241122-233644.pkl.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 116, in _wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1697, in execute_model
[rank0]:     output: SamplerOutput = self.model.sample(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 577, in sample
[rank0]:     next_tokens = self.sampler(logits, sampling_metadata)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 313, in forward
[rank0]:     prompt_logprobs, sample_logprobs = get_logprobs(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 1017, in get_logprobs
[rank0]:     logprobs[query_indices_gpu],
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.91 GiB. GPU 0 has a total capacity of 39.38 GiB of which 2.60 GiB is free. Including non-PyTorch memory, this process has 36.76 GiB memory in use. Of the allocated memory 36.11 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 58.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/bin/lm_eval", line 8, in <module>
[rank0]:     sys.exit(cli_evaluate())
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/__main__.py", line 375, in cli_evaluate
[rank0]:     results = evaluator.simple_evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 277, in simple_evaluate
[rank0]:     results = evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 449, in evaluate
[rank0]:     resps = getattr(lm, reqtype)(cloned_reqs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/api/model.py", line 371, in loglikelihood
[rank0]:     return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 443, in _loglikelihood_tokens
[rank0]:     outputs = self._model_generate(requests=inputs, generate=False)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 274, in _model_generate
[rank0]:     outputs = self.model.generate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/utils.py", line 1063, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 406, in generate
[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 942, in _run_engine
[rank0]:     step_outputs = self.llm_engine.step()
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 1454, in step
[rank0]:     outputs = self.model_executor.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 125, in execute_model
[rank0]:     output = self.driver_worker.execute_model(execute_model_req)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 343, in execute_model
[rank0]:     output = self.model_runner.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 152, in _wrapper
[rank0]:     raise type(err)(
[rank0]: torch.OutOfMemoryError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241122-233644.pkl): CUDA out of memory. Tried to allocate 2.91 GiB. GPU 0 has a total capacity of 39.38 GiB of which 2.60 GiB is free. Including non-PyTorch memory, this process has 36.76 GiB memory in use. Of the allocated memory 36.11 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 58.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
[rank0]:[W1122 23:36:44.955469076 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:36:47 UTC 2024
Fri Nov 22 23:37:03 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.7,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:37:16,394 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:37:16,394 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:37:28,135 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:37:28,135 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:37:28,142 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:37:28,142 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.7, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:37:39 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
INFO 11-22 23:37:39 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:37:41 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:37:41 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:37:42 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:37:42 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.92s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.92s/it]

INFO 11-22 23:37:45 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:37:45 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=23.53GiB gpu_memory_utilization=0.70
INFO 11-22 23:37:45 gpu_executor.py:113] # GPU blocks: 48194, # CPU blocks: 8192
INFO 11-22 23:37:45 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 94.13x
INFO 11-22 23:37:48 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:37:48 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:38:00 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-22:23:38:02,006 WARNING  [task.py:102] [meta_mmlu] passed `generation_kwargs`, but not using `output_type: generate_until`!
2024-11-22:23:38:05,227 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:38:05,240 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:38:06,851 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:38:06,855 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 27962.03it/s]
2024-11-22:23:38:06,868 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][AINFO 11-22 23:38:07 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20241122-233807.pkl...
INFO 11-22 23:38:07 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20241122-233807.pkl.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 116, in _wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1697, in execute_model
[rank0]:     output: SamplerOutput = self.model.sample(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 577, in sample
[rank0]:     next_tokens = self.sampler(logits, sampling_metadata)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 313, in forward
[rank0]:     prompt_logprobs, sample_logprobs = get_logprobs(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 1017, in get_logprobs
[rank0]:     logprobs[query_indices_gpu],
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.91 GiB. GPU 0 has a total capacity of 39.38 GiB of which 2.60 GiB is free. Including non-PyTorch memory, this process has 36.76 GiB memory in use. Of the allocated memory 36.11 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 58.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/bin/lm_eval", line 8, in <module>
[rank0]:     sys.exit(cli_evaluate())
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/__main__.py", line 375, in cli_evaluate
[rank0]:     results = evaluator.simple_evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 277, in simple_evaluate
[rank0]:     results = evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 449, in evaluate
[rank0]:     resps = getattr(lm, reqtype)(cloned_reqs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/api/model.py", line 371, in loglikelihood
[rank0]:     return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 443, in _loglikelihood_tokens
[rank0]:     outputs = self._model_generate(requests=inputs, generate=False)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 274, in _model_generate
[rank0]:     outputs = self.model.generate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/utils.py", line 1063, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 406, in generate
[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 942, in _run_engine
[rank0]:     step_outputs = self.llm_engine.step()
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 1454, in step
[rank0]:     outputs = self.model_executor.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 125, in execute_model
[rank0]:     output = self.driver_worker.execute_model(execute_model_req)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 343, in execute_model
[rank0]:     output = self.model_runner.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 152, in _wrapper
[rank0]:     raise type(err)(
[rank0]: torch.OutOfMemoryError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241122-233807.pkl): CUDA out of memory. Tried to allocate 2.91 GiB. GPU 0 has a total capacity of 39.38 GiB of which 2.60 GiB is free. Including non-PyTorch memory, this process has 36.76 GiB memory in use. Of the allocated memory 36.11 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 58.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
[rank0]:[W1122 23:38:07.839266036 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:38:10 UTC 2024
Fri Nov 22 23:38:12 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.6,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:38:24,884 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:38:24,885 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:38:36,480 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:38:36,481 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:38:36,489 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:38:36,489 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.6, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:38:48 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 11-22 23:38:48 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:38:49 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:38:50 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:38:51 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:38:51 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.92s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.92s/it]

INFO 11-22 23:38:53 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:38:54 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=19.59GiB gpu_memory_utilization=0.60
INFO 11-22 23:38:54 gpu_executor.py:113] # GPU blocks: 40129, # CPU blocks: 8192
INFO 11-22 23:38:54 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 78.38x
INFO 11-22 23:38:57 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:38:57 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:39:09 model_runner.py:1518] Graph capturing finished in 12 secs, took 0.14 GiB
2024-11-22:23:39:11,086 WARNING  [task.py:102] [meta_mmlu] passed `generation_kwargs`, but not using `output_type: generate_until`!
2024-11-22:23:39:14,266 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:39:14,279 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:39:15,900 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:39:15,904 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 27118.34it/s]
2024-11-22:23:39:15,917 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][AINFO 11-22 23:39:16 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20241122-233916.pkl...
INFO 11-22 23:39:16 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20241122-233916.pkl.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 116, in _wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1697, in execute_model
[rank0]:     output: SamplerOutput = self.model.sample(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 577, in sample
[rank0]:     next_tokens = self.sampler(logits, sampling_metadata)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 313, in forward
[rank0]:     prompt_logprobs, sample_logprobs = get_logprobs(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 1016, in get_logprobs
[rank0]:     ranks = _get_ranks(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 918, in _get_ranks
[rank0]:     return result.sum(1).add_(1)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.83 GiB. GPU 0 has a total capacity of 39.38 GiB of which 2.90 GiB is free. Including non-PyTorch memory, this process has 36.47 GiB memory in use. Of the allocated memory 35.81 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 60.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/bin/lm_eval", line 8, in <module>
[rank0]:     sys.exit(cli_evaluate())
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/__main__.py", line 375, in cli_evaluate
[rank0]:     results = evaluator.simple_evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 277, in simple_evaluate
[rank0]:     results = evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 449, in evaluate
[rank0]:     resps = getattr(lm, reqtype)(cloned_reqs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/api/model.py", line 371, in loglikelihood
[rank0]:     return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 443, in _loglikelihood_tokens
[rank0]:     outputs = self._model_generate(requests=inputs, generate=False)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 274, in _model_generate
[rank0]:     outputs = self.model.generate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/utils.py", line 1063, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 406, in generate
[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 942, in _run_engine
[rank0]:     step_outputs = self.llm_engine.step()
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 1454, in step
[rank0]:     outputs = self.model_executor.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 125, in execute_model
[rank0]:     output = self.driver_worker.execute_model(execute_model_req)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 343, in execute_model
[rank0]:     output = self.model_runner.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 152, in _wrapper
[rank0]:     raise type(err)(
[rank0]: torch.OutOfMemoryError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241122-233916.pkl): CUDA out of memory. Tried to allocate 5.83 GiB. GPU 0 has a total capacity of 39.38 GiB of which 2.90 GiB is free. Including non-PyTorch memory, this process has 36.47 GiB memory in use. Of the allocated memory 35.81 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 60.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
[rank0]:[W1122 23:39:16.923890557 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:39:19 UTC 2024
Fri Nov 22 23:39:47 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
bash: line 1: lm_eval: command not found
end: Fri Nov 22 23:39:47 UTC 2024
Fri Nov 22 23:39:58 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:40:11,541 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:40:11,541 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:40:23,056 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:40:23,056 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:40:23,063 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:40:23,063 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:40:33 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
INFO 11-22 23:40:33 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:40:34 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:40:35 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:40:36 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:40:36 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.91s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.91s/it]

INFO 11-22 23:40:38 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:40:39 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=31.41GiB gpu_memory_utilization=0.90
INFO 11-22 23:40:39 gpu_executor.py:113] # GPU blocks: 64324, # CPU blocks: 8192
INFO 11-22 23:40:39 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 125.63x
INFO 11-22 23:40:42 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:40:42 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:40:54 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-22:23:40:56,040 WARNING  [task.py:102] [meta_mmlu] passed `generation_kwargs`, but not using `output_type: generate_until`!
2024-11-22:23:40:58,787 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:40:58,800 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:41:00,438 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:41:00,441 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 28086.86it/s]
2024-11-22:23:41:00,454 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][AINFO 11-22 23:41:00 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20241122-234100.pkl...
INFO 11-22 23:41:00 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20241122-234100.pkl.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 116, in _wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1697, in execute_model
[rank0]:     output: SamplerOutput = self.model.sample(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 577, in sample
[rank0]:     next_tokens = self.sampler(logits, sampling_metadata)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 281, in forward
[rank0]:     probs = torch.softmax(logits, dim=-1, dtype=torch.float)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.91 GiB. GPU 0 has a total capacity of 39.38 GiB of which 569.38 MiB is free. Including non-PyTorch memory, this process has 38.81 GiB memory in use. Of the allocated memory 38.16 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 54.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/bin/lm_eval", line 8, in <module>
[rank0]:     sys.exit(cli_evaluate())
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/__main__.py", line 375, in cli_evaluate
[rank0]:     results = evaluator.simple_evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 277, in simple_evaluate
[rank0]:     results = evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 449, in evaluate
[rank0]:     resps = getattr(lm, reqtype)(cloned_reqs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/api/model.py", line 371, in loglikelihood
[rank0]:     return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 443, in _loglikelihood_tokens
[rank0]:     outputs = self._model_generate(requests=inputs, generate=False)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 274, in _model_generate
[rank0]:     outputs = self.model.generate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/utils.py", line 1063, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 406, in generate
[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 942, in _run_engine
[rank0]:     step_outputs = self.llm_engine.step()
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 1454, in step
[rank0]:     outputs = self.model_executor.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 125, in execute_model
[rank0]:     output = self.driver_worker.execute_model(execute_model_req)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 343, in execute_model
[rank0]:     output = self.model_runner.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 152, in _wrapper
[rank0]:     raise type(err)(
[rank0]: torch.OutOfMemoryError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241122-234100.pkl): CUDA out of memory. Tried to allocate 2.91 GiB. GPU 0 has a total capacity of 39.38 GiB of which 569.38 MiB is free. Including non-PyTorch memory, this process has 38.81 GiB memory in use. Of the allocated memory 38.16 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 54.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
[rank0]:[W1122 23:41:01.453063668 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:41:03 UTC 2024
Fri Nov 22 23:41:21 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:41:34,030 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:41:34,030 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:41:46,190 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:41:46,191 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:41:46,197 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:41:46,197 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.5, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:41:57 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 11-22 23:41:57 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:41:58 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:41:59 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:41:59 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:42:00 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.91s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.91s/it]

INFO 11-22 23:42:02 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:42:02 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=15.66GiB gpu_memory_utilization=0.50
INFO 11-22 23:42:03 gpu_executor.py:113] # GPU blocks: 32063, # CPU blocks: 8192
INFO 11-22 23:42:03 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 62.62x
INFO 11-22 23:42:06 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:42:06 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:42:17 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-22:23:42:19,279 WARNING  [task.py:102] [meta_mmlu] passed `generation_kwargs`, but not using `output_type: generate_until`!
2024-11-22:23:42:22,343 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:42:22,356 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:42:24,199 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:42:24,202 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 28468.13it/s]
2024-11-22:23:42:24,215 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   8%|▊         | 1/12 [00:00<00:03,  3.23it/s, est. speed input: 1708.17 toks/s, output: 3.23 toks/s][AProcessed prompts: 100%|██████████| 12/12 [00:00<00:00, 38.49it/s, est. speed input: 19564.67 toks/s, output: 38.51 toks/s]
Running loglikelihood requests:   8%|▊         | 1/12 [00:00<00:03,  3.17it/s]Running loglikelihood requests: 100%|██████████| 12/12 [00:00<00:00, 37.41it/s]
2024-11-22:23:42:24,576 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'Two whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?', 'input_correct_responses': ['Answer: C'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': '2f9f3789c31bccfee18249d4e2b191cf856e5b1ef05272817f7703c85f7bca7f', 'input_choice_list': {'A': '6 and 10', 'B': '5 and 12', 'C': '10 and 12', 'D': '12 and 15'}, 'output_prediction_text': None, 'problem': 'Two whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?', 'gold': 'Answer: C'}


2024-11-22:23:42:24,577 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'Evaluate −2(x − 3) for x = 2.', 'input_correct_responses': ['Answer: D'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': '3260939534b25679362acf409872c321ede427a5117d95bf4eb2cb1d11575b8f', 'input_choice_list': {'A': '−4', 'B': '−2', 'C': '10', 'D': '2'}, 'output_prediction_text': None, 'problem': 'Evaluate −2(x − 3) for x = 2.', 'gold': 'Answer: D'}


2024-11-22:23:42:24,577 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'There are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?', 'input_correct_responses': ['Answer: C'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': 'b1270d349f78638b9ed0e54b6e8d7d007a4759c3c882e9fb8698f6ff7ca1b066', 'input_choice_list': {'A': '232', 'B': '236', 'C': '345', 'D': '460'}, 'output_prediction_text': None, 'problem': 'There are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?', 'gold': 'Answer: C'}


2024-11-22:23:42:26,874 INFO     [evaluation_tracker.py:182] Saving results aggregated
2024-11-22:23:42:26,882 INFO     [evaluation_tracker.py:258] Saving per-sample results for: meta_mmlu
vllm (pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42), gen_kwargs: (None), limit: 3.0, num_fewshot: None, batch_size: auto
|    Tasks    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
| - meta_mmlu |      1|none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

|   Groups    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

[rank0]:[W1122 23:42:27.788063536 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:42:30 UTC 2024
Fri Nov 22 23:45:30 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:45:43,534 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:45:43,534 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:45:55,492 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:45:55,493 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:45:55,500 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:45:55,500 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.5, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:46:07 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 11-22 23:46:07 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:46:08 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:46:08 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:46:09 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:46:09 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.92s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.92s/it]

INFO 11-22 23:46:12 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:46:12 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=15.66GiB gpu_memory_utilization=0.50
INFO 11-22 23:46:12 gpu_executor.py:113] # GPU blocks: 32063, # CPU blocks: 8192
INFO 11-22 23:46:12 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 62.62x
INFO 11-22 23:46:15 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:46:15 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:46:27 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-22:23:46:28,551 WARNING  [task.py:102] [meta_mmlu] passed `generation_kwargs`, but not using `output_type: generate_until`!
2024-11-22:23:46:31,596 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:46:31,608 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:46:33,465 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:46:33,468 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 28793.85it/s]
2024-11-22:23:46:33,481 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   8%|▊         | 1/12 [00:00<00:02,  4.29it/s, est. speed input: 2272.09 toks/s, output: 4.29 toks/s][AProcessed prompts: 100%|██████████| 12/12 [00:00<00:00, 51.09it/s, est. speed input: 25972.87 toks/s, output: 51.11 toks/s]
Running loglikelihood requests:   8%|▊         | 1/12 [00:00<00:02,  4.19it/s]Running loglikelihood requests: 100%|██████████| 12/12 [00:00<00:00, 49.21it/s]
2024-11-22:23:46:33,765 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'Two whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?', 'input_correct_responses': ['Answer: C'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': '2f9f3789c31bccfee18249d4e2b191cf856e5b1ef05272817f7703c85f7bca7f', 'input_choice_list': {'A': '6 and 10', 'B': '5 and 12', 'C': '10 and 12', 'D': '12 and 15'}, 'output_prediction_text': None, 'problem': 'Two whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?', 'gold': 'Answer: C'}


2024-11-22:23:46:33,766 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'Evaluate −2(x − 3) for x = 2.', 'input_correct_responses': ['Answer: D'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': '3260939534b25679362acf409872c321ede427a5117d95bf4eb2cb1d11575b8f', 'input_choice_list': {'A': '−4', 'B': '−2', 'C': '10', 'D': '2'}, 'output_prediction_text': None, 'problem': 'Evaluate −2(x − 3) for x = 2.', 'gold': 'Answer: D'}


2024-11-22:23:46:33,767 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'There are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?', 'input_correct_responses': ['Answer: C'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': 'b1270d349f78638b9ed0e54b6e8d7d007a4759c3c882e9fb8698f6ff7ca1b066', 'input_choice_list': {'A': '232', 'B': '236', 'C': '345', 'D': '460'}, 'output_prediction_text': None, 'problem': 'There are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?', 'gold': 'Answer: C'}


2024-11-22:23:46:35,942 INFO     [evaluation_tracker.py:182] Saving results aggregated
2024-11-22:23:46:35,950 INFO     [evaluation_tracker.py:258] Saving per-sample results for: meta_mmlu
vllm (pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42), gen_kwargs: (None), limit: 3.0, num_fewshot: None, batch_size: auto
|    Tasks    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
| - meta_mmlu |      1|none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

|   Groups    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

[rank0]:[W1122 23:46:36.844476975 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:46:39 UTC 2024
Fri Nov 22 23:47:48 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:48:01,005 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:48:01,005 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:48:12,696 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:48:12,696 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:48:12,703 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:48:12,704 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.5, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:48:24 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 11-22 23:48:24 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:48:25 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:48:25 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:48:26 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:48:26 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.90s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.90s/it]

INFO 11-22 23:48:29 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:48:29 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=15.66GiB gpu_memory_utilization=0.50
INFO 11-22 23:48:29 gpu_executor.py:113] # GPU blocks: 32063, # CPU blocks: 8192
INFO 11-22 23:48:29 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 62.62x
INFO 11-22 23:48:33 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:48:33 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:48:44 model_runner.py:1518] Graph capturing finished in 12 secs, took 0.14 GiB
2024-11-22:23:48:49,387 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:48:49,400 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:48:51,247 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:48:51,250 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 28926.23it/s]
2024-11-22:23:48:51,263 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   8%|▊         | 1/12 [00:00<00:02,  4.29it/s, est. speed input: 2272.23 toks/s, output: 4.29 toks/s][AProcessed prompts: 100%|██████████| 12/12 [00:00<00:00, 51.08it/s, est. speed input: 25966.62 toks/s, output: 51.10 toks/s]
Running loglikelihood requests:   8%|▊         | 1/12 [00:00<00:02,  4.18it/s]Running loglikelihood requests: 100%|██████████| 12/12 [00:00<00:00, 49.17it/s]
2024-11-22:23:48:51,547 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'Two whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?', 'input_correct_responses': ['Answer: C'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': '2f9f3789c31bccfee18249d4e2b191cf856e5b1ef05272817f7703c85f7bca7f', 'input_choice_list': {'A': '6 and 10', 'B': '5 and 12', 'C': '10 and 12', 'D': '12 and 15'}, 'output_prediction_text': None, 'problem': 'Two whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?', 'gold': 'Answer: C'}


2024-11-22:23:48:51,548 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'Evaluate −2(x − 3) for x = 2.', 'input_correct_responses': ['Answer: D'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': '3260939534b25679362acf409872c321ede427a5117d95bf4eb2cb1d11575b8f', 'input_choice_list': {'A': '−4', 'B': '−2', 'C': '10', 'D': '2'}, 'output_prediction_text': None, 'problem': 'Evaluate −2(x − 3) for x = 2.', 'gold': 'Answer: D'}


2024-11-22:23:48:51,549 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'There are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?', 'input_correct_responses': ['Answer: C'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': 'b1270d349f78638b9ed0e54b6e8d7d007a4759c3c882e9fb8698f6ff7ca1b066', 'input_choice_list': {'A': '232', 'B': '236', 'C': '345', 'D': '460'}, 'output_prediction_text': None, 'problem': 'There are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?', 'gold': 'Answer: C'}


2024-11-22:23:48:54,117 INFO     [evaluation_tracker.py:182] Saving results aggregated
2024-11-22:23:48:54,124 INFO     [evaluation_tracker.py:258] Saving per-sample results for: meta_mmlu
vllm (pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42), gen_kwargs: (None), limit: 3.0, num_fewshot: None, batch_size: auto
|    Tasks    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
| - meta_mmlu |      1|none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

|   Groups    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

[rank0]:[W1122 23:48:55.058401691 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:48:57 UTC 2024
Fri Nov 22 23:50:38 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:50:51,281 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:50:51,281 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:51:02,506 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:51:02,507 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:51:02,515 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:51:02,515 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.5, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:51:13 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
INFO 11-22 23:51:13 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:51:14 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:51:15 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:51:15 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:51:16 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.93s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.93s/it]

INFO 11-22 23:51:18 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:51:18 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=15.66GiB gpu_memory_utilization=0.50
INFO 11-22 23:51:19 gpu_executor.py:113] # GPU blocks: 32063, # CPU blocks: 8192
INFO 11-22 23:51:19 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 62.62x
INFO 11-22 23:51:22 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:51:22 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:51:34 model_runner.py:1518] Graph capturing finished in 12 secs, took 0.14 GiB
2024-11-22:23:51:38,481 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:51:38,494 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:51:40,315 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:51:40,317 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 28403.86it/s]
2024-11-22:23:51:40,330 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   8%|▊         | 1/12 [00:00<00:02,  4.31it/s, est. speed input: 2275.44 toks/s, output: 4.31 toks/s][AProcessed prompts: 100%|██████████| 12/12 [00:00<00:00, 51.26it/s, est. speed input: 26004.75 toks/s, output: 51.28 toks/s]
Running loglikelihood requests:   8%|▊         | 1/12 [00:00<00:02,  4.20it/s]Running loglikelihood requests: 100%|██████████| 12/12 [00:00<00:00, 49.33it/s]
2024-11-22:23:51:40,614 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'Two whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?', 'input_correct_responses': ['Answer: C'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': '2f9f3789c31bccfee18249d4e2b191cf856e5b1ef05272817f7703c85f7bca7f', 'input_choice_list': {'A': '6 and 10', 'B': '5 and 12', 'C': '10 and 12', 'D': '12 and 15'}, 'output_prediction_text': None, 'problem': 'Two whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?', 'gold': 'Answer: C'}


2024-11-22:23:51:40,614 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'Evaluate −2(x − 3) for x = 2.', 'input_correct_responses': ['Answer: D'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': '3260939534b25679362acf409872c321ede427a5117d95bf4eb2cb1d11575b8f', 'input_choice_list': {'A': '−4', 'B': '−2', 'C': '10', 'D': '2'}, 'output_prediction_text': None, 'problem': 'Evaluate −2(x − 3) for x = 2.', 'gold': 'Answer: D'}


2024-11-22:23:51:40,615 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'There are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?', 'input_correct_responses': ['Answer: C'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': 'b1270d349f78638b9ed0e54b6e8d7d007a4759c3c882e9fb8698f6ff7ca1b066', 'input_choice_list': {'A': '232', 'B': '236', 'C': '345', 'D': '460'}, 'output_prediction_text': None, 'problem': 'There are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?', 'gold': 'Answer: C'}


2024-11-22:23:51:42,872 INFO     [evaluation_tracker.py:182] Saving results aggregated
2024-11-22:23:51:42,877 INFO     [evaluation_tracker.py:258] Saving per-sample results for: meta_mmlu
vllm (pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42), gen_kwargs: (None), limit: 3.0, num_fewshot: None, batch_size: auto
|    Tasks    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
| - meta_mmlu |      1|none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

|   Groups    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

[rank0]:[W1122 23:51:43.768729355 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:51:46 UTC 2024
Fri Nov 22 23:54:37 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:54:49,543 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:54:49,543 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:55:01,152 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:55:01,153 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:55:01,160 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:55:01,160 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.5, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:55:12 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 11-22 23:55:12 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:55:13 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:55:13 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:55:14 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:55:14 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.87s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.87s/it]

INFO 11-22 23:55:16 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:55:17 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=15.66GiB gpu_memory_utilization=0.50
INFO 11-22 23:55:17 gpu_executor.py:113] # GPU blocks: 32063, # CPU blocks: 8192
INFO 11-22 23:55:17 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 62.62x
INFO 11-22 23:55:20 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:55:20 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:55:32 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-22:23:55:36,988 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
Map:   0%|          | 0/14042 [00:00<?, ? examples/s]Map:   7%|▋         | 1000/14042 [00:00<00:01, 8534.93 examples/s]Map:  14%|█▍        | 2000/14042 [00:00<00:01, 8951.85 examples/s]Map:  21%|██▏       | 3000/14042 [00:00<00:02, 4871.38 examples/s]Map:  28%|██▊       | 4000/14042 [00:00<00:01, 6027.07 examples/s]Map:  37%|███▋      | 5158/14042 [00:00<00:01, 7435.52 examples/s]Map:  45%|████▌     | 6359/14042 [00:00<00:00, 8650.63 examples/s]Map:  56%|█████▌    | 7802/14042 [00:00<00:00, 9751.25 examples/s]Map:  65%|██████▌   | 9171/14042 [00:01<00:00, 9462.48 examples/s]Map:  77%|███████▋  | 10825/14042 [00:01<00:00, 10216.84 examples/s]Map:  85%|████████▌ | 12000/14042 [00:01<00:00, 10004.29 examples/s]Map:  93%|█████████▎| 13097/14042 [00:01<00:00, 10244.53 examples/s]Map: 100%|██████████| 14042/14042 [00:01<00:00, 8605.85 examples/s] 
Map:   0%|          | 0/14042 [00:00<?, ? examples/s]Map:   7%|▋         | 1000/14042 [00:00<00:01, 9475.38 examples/s]Map:  14%|█▍        | 2000/14042 [00:00<00:01, 9561.43 examples/s]Map:  21%|██▏       | 3019/14042 [00:00<00:01, 9841.28 examples/s]Map:  34%|███▎      | 4725/14042 [00:00<00:00, 10363.22 examples/s]Map:  41%|████      | 5783/14042 [00:00<00:00, 10407.64 examples/s]Map:  49%|████▉     | 6856/14042 [00:00<00:00, 10504.73 examples/s]Map:  57%|█████▋    | 8005/14042 [00:00<00:00, 9369.30 examples/s] Map:  64%|██████▍   | 9054/14042 [00:00<00:00, 9674.87 examples/s]Map:  76%|███████▋  | 10726/14042 [00:01<00:00, 10141.24 examples/s]Map:  84%|████████▍ | 11821/14042 [00:01<00:00, 10348.70 examples/s]Map:  92%|█████████▏| 12895/14042 [00:01<00:00, 10452.28 examples/s]Map: 100%|██████████| 14042/14042 [00:01<00:00, 7812.16 examples/s] Map: 100%|██████████| 14042/14042 [00:01<00:00, 9291.41 examples/s]
2024-11-22:23:55:40,151 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:55:41,772 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:55:41,775 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 23831.27it/s]
2024-11-22:23:55:41,788 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   8%|▊         | 1/12 [00:00<00:02,  4.27it/s, est. speed input: 2257.47 toks/s, output: 4.27 toks/s][AProcessed prompts: 100%|██████████| 12/12 [00:00<00:00, 50.86it/s, est. speed input: 25807.06 toks/s, output: 50.89 toks/s]
Running loglikelihood requests:   8%|▊         | 1/12 [00:00<00:02,  4.17it/s]Running loglikelihood requests: 100%|██████████| 12/12 [00:00<00:00, 49.01it/s]
2024-11-22:23:55:42,074 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'Two whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?', 'input_correct_responses': ['Answer: C'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': '2f9f3789c31bccfee18249d4e2b191cf856e5b1ef05272817f7703c85f7bca7f', 'input_choice_list': {'A': '6 and 10', 'B': '5 and 12', 'C': '10 and 12', 'D': '12 and 15'}, 'output_prediction_text': None, 'problem': 'Two whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?', 'response': 'C'}


2024-11-22:23:55:42,075 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'Evaluate −2(x − 3) for x = 2.', 'input_correct_responses': ['Answer: D'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': '3260939534b25679362acf409872c321ede427a5117d95bf4eb2cb1d11575b8f', 'input_choice_list': {'A': '−4', 'B': '−2', 'C': '10', 'D': '2'}, 'output_prediction_text': None, 'problem': 'Evaluate −2(x − 3) for x = 2.', 'response': 'D'}


2024-11-22:23:55:42,077 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'There are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?', 'input_correct_responses': ['Answer: C'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': 'b1270d349f78638b9ed0e54b6e8d7d007a4759c3c882e9fb8698f6ff7ca1b066', 'input_choice_list': {'A': '232', 'B': '236', 'C': '345', 'D': '460'}, 'output_prediction_text': None, 'problem': 'There are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?', 'response': 'C'}


2024-11-22:23:55:44,365 INFO     [evaluation_tracker.py:182] Saving results aggregated
2024-11-22:23:55:44,371 INFO     [evaluation_tracker.py:258] Saving per-sample results for: meta_mmlu
vllm (pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42), gen_kwargs: (None), limit: 3.0, num_fewshot: None, batch_size: auto
|    Tasks    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
| - meta_mmlu |      1|none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

|   Groups    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

[rank0]:[W1122 23:55:45.271652003 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:55:47 UTC 2024
Fri Nov 22 23:58:14 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
bash: line 1: lm_eval: command not found
end: Fri Nov 22 23:58:14 UTC 2024
Fri Nov 22 23:58:22 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
bash: line 1: lm_eval: command not found
end: Fri Nov 22 23:58:22 UTC 2024
Fri Nov 22 23:58:32 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
bash: line 1: lm_eval: command not found
end: Fri Nov 22 23:58:32 UTC 2024
Fri Nov 22 23:58:44 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-22:23:58:56,956 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:23:58:56,956 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:23:59:08,581 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-22:23:59:08,582 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:23:59:08,588 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:23:59:08,588 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.5, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-22 23:59:21 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 11-22 23:59:21 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-22 23:59:22 selector.py:135] Using Flash Attention backend.
INFO 11-22 23:59:22 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-22 23:59:23 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-22 23:59:23 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.89s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.89s/it]

INFO 11-22 23:59:26 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-22 23:59:26 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=15.66GiB gpu_memory_utilization=0.50
INFO 11-22 23:59:26 gpu_executor.py:113] # GPU blocks: 32063, # CPU blocks: 8192
INFO 11-22 23:59:26 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 62.62x
INFO 11-22 23:59:30 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-22 23:59:30 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-22 23:59:41 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-22:23:59:46,325 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
Map:   0%|          | 0/14042 [00:00<?, ? examples/s]Map:   7%|▋         | 1000/14042 [00:00<00:01, 8701.01 examples/s]Map:  14%|█▍        | 2000/14042 [00:00<00:01, 9099.49 examples/s]Map:  21%|██▏       | 3000/14042 [00:00<00:02, 4856.54 examples/s]Map:  28%|██▊       | 4000/14042 [00:00<00:01, 6031.99 examples/s]Map:  37%|███▋      | 5151/14042 [00:00<00:01, 7421.03 examples/s]Map:  45%|████▌     | 6363/14042 [00:00<00:00, 8669.78 examples/s]Map:  56%|█████▌    | 7809/14042 [00:00<00:00, 9777.88 examples/s]Map:  65%|██████▌   | 9163/14042 [00:01<00:00, 9433.67 examples/s]Map:  78%|███████▊  | 10972/14042 [00:01<00:00, 10319.95 examples/s]Map:  91%|█████████▏| 12816/14042 [00:01<00:00, 10653.37 examples/s]Map:  99%|█████████▉| 13949/14042 [00:01<00:00, 10808.85 examples/s]Map: 100%|██████████| 14042/14042 [00:01<00:00, 8690.79 examples/s] 
Map:   0%|          | 0/14042 [00:00<?, ? examples/s]Map:   7%|▋         | 1000/14042 [00:00<00:01, 9493.46 examples/s]Map:  14%|█▍        | 2000/14042 [00:00<00:01, 9658.52 examples/s]Map:  22%|██▏       | 3031/14042 [00:00<00:01, 9948.63 examples/s]Map:  34%|███▎      | 4731/14042 [00:00<00:00, 10470.08 examples/s]Map:  41%|████▏     | 5800/14042 [00:00<00:00, 10535.32 examples/s]Map:  49%|████▉     | 6884/14042 [00:00<00:00, 10627.60 examples/s]Map:  57%|█████▋    | 8050/14042 [00:00<00:00, 9484.37 examples/s] Map:  65%|██████▍   | 9110/14042 [00:00<00:00, 9789.82 examples/s]Map:  77%|███████▋  | 10783/14042 [00:01<00:00, 10292.65 examples/s]Map:  85%|████████▍ | 11892/14042 [00:01<00:00, 10498.60 examples/s]Map:  93%|█████████▎| 12997/14042 [00:01<00:00, 10647.21 examples/s]Map: 100%|██████████| 14042/14042 [00:01<00:00, 9715.92 examples/s] 
2024-11-22:23:59:49,404 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-22:23:59:50,996 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-22:23:59:50,999 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 29059.84it/s]
2024-11-22:23:59:51,012 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   8%|▊         | 1/12 [00:00<00:02,  4.31it/s, est. speed input: 2275.48 toks/s, output: 4.31 toks/s][AProcessed prompts: 100%|██████████| 12/12 [00:00<00:00, 51.27it/s, est. speed input: 26013.44 toks/s, output: 51.30 toks/s]
Running loglikelihood requests:   8%|▊         | 1/12 [00:00<00:02,  4.20it/s]Running loglikelihood requests: 100%|██████████| 12/12 [00:00<00:00, 49.41it/s]
2024-11-22:23:59:51,296 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'Two whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?', 'input_correct_responses': ['Answer: C'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nTwo whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?\nA. 6 and 10\nB. 5 and 12\nC. 10 and 12\nD. 12 and 15\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': '2f9f3789c31bccfee18249d4e2b191cf856e5b1ef05272817f7703c85f7bca7f', 'input_choice_list': {'A': '6 and 10', 'B': '5 and 12', 'C': '10 and 12', 'D': '12 and 15'}, 'output_prediction_text': None, 'problem': 'Two whole numbers have a least common multiple of 60. Each number is less than or equal to 12. The greatest common factor of the two numbersis 2. What are the two numbers?', 'answer': 'C'}


2024-11-22:23:59:51,297 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'Evaluate −2(x − 3) for x = 2.', 'input_correct_responses': ['Answer: D'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nEvaluate −2(x − 3) for x = 2.\nA. −4\nB. −2\nC. 10\nD. 2\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': '3260939534b25679362acf409872c321ede427a5117d95bf4eb2cb1d11575b8f', 'input_choice_list': {'A': '−4', 'B': '−2', 'C': '10', 'D': '2'}, 'output_prediction_text': None, 'problem': 'Evaluate −2(x − 3) for x = 2.', 'answer': 'D'}


2024-11-22:23:59:51,299 WARNING  [task.py:1399] Label index was not in within range of available choices,Sample:

{'input_question': 'There are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?', 'input_correct_responses': ['Answer: C'], 'input_final_prompts': ['The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: A', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: B', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: C', 'The following are multiple choice questions (with answers) about elementary mathematics.\n\nThe population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826?\nA. 5 thousands\nB. 5 hundreds\nC. 5 tens\nD. 5 ones\nAnswer: A\n\nOlivia used the rule "Add 11" to create the number pattern shown below. 10, 21, 32, 43, 54 Which statement about the number pattern is true?\nA. The 10th number in the pattern will be an even number.\nB. The number pattern will never have two even numbers next to each other.\nC. The next two numbers in the pattern will be an even number then an odd number.\nD. If the number pattern started with an odd number then the pattern would have only odd numbers in it.\nAnswer: B\n\nA total of 30 players will play basketball at a park. There will be exactly 5 players on each team. Which statement correctly explains how to find the number of teams needed?\nA. Add 5 to 30 to find 35 teams.\nB. Divide 30 by 5 to find 6 teams.\nC. Multiply 30 and 5 to find 150 teams.\nD. Subtract 5 from 30 to find 25 teams.\nAnswer: B\n\nA store sells 107 different colors of paint. They have 25 cans of each color in storage. The number of cans of paint the store has in storage can be found using the expression below. 107 × 25. How many cans of paint does the store have in storage?\nA. 749\nB. 2,675\nC. 2,945\nD. 4,250\nAnswer: B\n\nWhich expression is equivalent to 5 x 9?\nA. (5 x 4) x (6 x 5)\nB. (5 x 5) + (5 x 4)\nC. (5 x 5) + (5 x 9)\nD. (5 x 9) x (6 x 9)\nAnswer: B\n\nThere are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?\nA. 232\nB. 236\nC. 345\nD. 460\nAnswer: D'], 'previously_is_correct': False, 'input_question_hash': 'b1270d349f78638b9ed0e54b6e8d7d007a4759c3c882e9fb8698f6ff7ca1b066', 'input_choice_list': {'A': '232', 'B': '236', 'C': '345', 'D': '460'}, 'output_prediction_text': None, 'problem': 'There are 230 calories in 4 ounces of a type of ice cream. How many calories are in 6 ounces of that ice cream?', 'answer': 'C'}


2024-11-22:23:59:53,617 INFO     [evaluation_tracker.py:182] Saving results aggregated
2024-11-22:23:59:53,623 INFO     [evaluation_tracker.py:258] Saving per-sample results for: meta_mmlu
vllm (pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42), gen_kwargs: (None), limit: 3.0, num_fewshot: None, batch_size: auto
|    Tasks    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
| - meta_mmlu |      1|none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

|   Groups    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

[rank0]:[W1122 23:59:54.523793426 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Fri Nov 22 23:59:56 UTC 2024
Sat Nov 23 00:01:24 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-23:00:01:36,873 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-23:00:01:36,873 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-23:00:01:48,203 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-23:00:01:48,203 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-23:00:01:48,211 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-23:00:01:48,211 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.5, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-23 00:01:59 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 11-23 00:01:59 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-23 00:02:00 selector.py:135] Using Flash Attention backend.
INFO 11-23 00:02:01 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-23 00:02:02 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-23 00:02:02 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.88s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.88s/it]

INFO 11-23 00:02:04 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-23 00:02:05 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=15.66GiB gpu_memory_utilization=0.50
INFO 11-23 00:02:05 gpu_executor.py:113] # GPU blocks: 32063, # CPU blocks: 8192
INFO 11-23 00:02:05 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 62.62x
INFO 11-23 00:02:08 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-23 00:02:08 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-23 00:02:20 model_runner.py:1518] Graph capturing finished in 12 secs, took 0.14 GiB
2024-11-23:00:02:25,319 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-23:00:02:25,332 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-23:00:02:27,163 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-23:00:02:27,166 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 28859.89it/s]
2024-11-23:00:02:27,179 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   8%|▊         | 1/12 [00:00<00:02,  4.30it/s, est. speed input: 2269.89 toks/s, output: 4.30 toks/s][AProcessed prompts: 100%|██████████| 12/12 [00:00<00:00, 51.14it/s, est. speed input: 25945.81 toks/s, output: 51.16 toks/s]
Running loglikelihood requests:   8%|▊         | 1/12 [00:00<00:02,  4.19it/s]Running loglikelihood requests: 100%|██████████| 12/12 [00:00<00:00, 49.24it/s]
2024-11-23:00:02:29,753 INFO     [evaluation_tracker.py:182] Saving results aggregated
2024-11-23:00:02:29,761 INFO     [evaluation_tracker.py:258] Saving per-sample results for: meta_mmlu
vllm (pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42), gen_kwargs: (None), limit: 3.0, num_fewshot: None, batch_size: auto
|    Tasks    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
| - meta_mmlu |      1|none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

|   Groups    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

[rank0]:[W1123 00:02:30.665951115 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Sat Nov 23 00:02:33 UTC 2024
Sat Nov 23 00:10:19 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
bash: line 1: lm_eval: command not found
end: Sat Nov 23 00:10:19 UTC 2024
Sat Nov 23 00:10:55 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 3
nohup: ignoring input
2024-11-23:00:11:09,294 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-23:00:11:09,294 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-23:00:11:21,058 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-23:00:11:21,059 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-23:00:11:21,066 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-23:00:11:21,066 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.5, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-23 00:11:32 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
INFO 11-23 00:11:32 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-23 00:11:33 selector.py:135] Using Flash Attention backend.
INFO 11-23 00:11:34 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-23 00:11:35 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-23 00:11:35 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.92s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.92s/it]

INFO 11-23 00:11:37 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-23 00:11:38 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=15.66GiB gpu_memory_utilization=0.50
INFO 11-23 00:11:38 gpu_executor.py:113] # GPU blocks: 32063, # CPU blocks: 8192
INFO 11-23 00:11:38 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 62.62x
INFO 11-23 00:11:41 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-23 00:11:41 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-23 00:11:53 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-23:00:11:57,459 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
Map:   0%|          | 0/14042 [00:00<?, ? examples/s]Map:   7%|▋         | 1000/14042 [00:00<00:01, 8601.51 examples/s]Map:  14%|█▍        | 2000/14042 [00:00<00:01, 9030.20 examples/s]Map:  21%|██▏       | 3000/14042 [00:00<00:02, 4939.52 examples/s]Map:  28%|██▊       | 4000/14042 [00:00<00:01, 6082.87 examples/s]Map:  36%|███▋      | 5111/14042 [00:00<00:01, 7365.55 examples/s]Map:  45%|████▍     | 6290/14042 [00:00<00:00, 8547.11 examples/s]Map:  56%|█████▌    | 7798/14042 [00:00<00:00, 9702.45 examples/s]Map:  65%|██████▌   | 9147/14042 [00:01<00:00, 9323.37 examples/s]Map:  78%|███████▊  | 10891/14042 [00:01<00:00, 10092.46 examples/s]Map:  85%|████████▌ | 12000/14042 [00:01<00:00, 9831.69 examples/s] Map:  93%|█████████▎| 13088/14042 [00:01<00:00, 10087.86 examples/s]Map: 100%|██████████| 14042/14042 [00:01<00:00, 8601.35 examples/s] 
Map:   0%|          | 0/14042 [00:00<?, ? examples/s]Map:   7%|▋         | 1000/14042 [00:00<00:01, 9275.97 examples/s]Map:  14%|█▍        | 2000/14042 [00:00<00:01, 9317.33 examples/s]Map:  21%|██▏       | 3000/14042 [00:00<00:01, 9536.80 examples/s]Map:  28%|██▊       | 4000/14042 [00:00<00:01, 9270.55 examples/s]Map:  36%|███▌      | 5009/14042 [00:00<00:00, 9550.98 examples/s]Map:  43%|████▎     | 6028/14042 [00:00<00:00, 9758.52 examples/s]Map:  55%|█████▍    | 7707/14042 [00:00<00:00, 10349.06 examples/s]Map:  64%|██████▍   | 9004/14042 [00:00<00:00, 9389.55 examples/s] Map:  71%|███████   | 10000/14042 [00:01<00:00, 9247.39 examples/s]Map:  79%|███████▊  | 11031/14042 [00:01<00:00, 9521.77 examples/s]Map:  86%|████████▌ | 12069/14042 [00:01<00:00, 9754.48 examples/s]Map:  98%|█████████▊| 13712/14042 [00:01<00:00, 10191.53 examples/s]Map: 100%|██████████| 14042/14042 [00:01<00:00, 9290.68 examples/s] 
2024-11-23:00:12:00,622 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-23:00:12:02,247 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-23:00:12:02,250 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 28532.68it/s]
2024-11-23:00:12:02,263 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/12 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/12 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   8%|▊         | 1/12 [00:00<00:02,  4.30it/s, est. speed input: 2272.75 toks/s, output: 4.30 toks/s][AProcessed prompts: 100%|██████████| 12/12 [00:00<00:00, 51.24it/s, est. speed input: 25998.39 toks/s, output: 51.27 toks/s]
Running loglikelihood requests:   8%|▊         | 1/12 [00:00<00:02,  4.19it/s]Running loglikelihood requests: 100%|██████████| 12/12 [00:00<00:00, 49.28it/s]
2024-11-23:00:12:04,828 INFO     [evaluation_tracker.py:182] Saving results aggregated
2024-11-23:00:12:04,836 INFO     [evaluation_tracker.py:258] Saving per-sample results for: meta_mmlu
vllm (pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42), gen_kwargs: (None), limit: 3.0, num_fewshot: None, batch_size: auto
|    Tasks    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
| - meta_mmlu |      1|none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

|   Groups    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |    0|±  |     0|
|             |       |none  |     0|acc_norm|↑  |    0|±  |     0|

[rank0]:[W1123 00:12:05.888278568 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Sat Nov 23 00:12:08 UTC 2024
Sat Nov 23 00:13:38 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 10
nohup: ignoring input
bash: line 1: lm_eval: command not found
end: Sat Nov 23 00:13:38 UTC 2024
Sat Nov 23 00:13:44 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 10
nohup: ignoring input
bash: line 1: lm_eval: command not found
end: Sat Nov 23 00:13:44 UTC 2024
Sat Nov 23 00:13:58 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.5,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 10
nohup: ignoring input
2024-11-23:00:14:12,450 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-23:00:14:12,450 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-23:00:14:24,201 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-23:00:14:24,202 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-23:00:14:24,209 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-23:00:14:24,209 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.5, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-23 00:14:35 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
INFO 11-23 00:14:35 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-23 00:14:36 selector.py:135] Using Flash Attention backend.
INFO 11-23 00:14:37 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-23 00:14:37 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-23 00:14:38 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.90s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.90s/it]

INFO 11-23 00:14:40 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-23 00:14:41 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=15.66GiB gpu_memory_utilization=0.50
INFO 11-23 00:14:41 gpu_executor.py:113] # GPU blocks: 32063, # CPU blocks: 8192
INFO 11-23 00:14:41 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 62.62x
INFO 11-23 00:14:44 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-23 00:14:44 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-23 00:14:55 model_runner.py:1518] Graph capturing finished in 11 secs, took 0.14 GiB
2024-11-23:00:15:00,574 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-23:00:15:00,587 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-23:00:15:02,462 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-23:00:15:02,465 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 43018.50it/s]
2024-11-23:00:15:02,479 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/40 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/40 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][AINFO 11-23 00:15:02 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20241123-001502.pkl...
INFO 11-23 00:15:02 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20241123-001502.pkl.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 116, in _wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1697, in execute_model
[rank0]:     output: SamplerOutput = self.model.sample(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 577, in sample
[rank0]:     next_tokens = self.sampler(logits, sampling_metadata)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 313, in forward
[rank0]:     prompt_logprobs, sample_logprobs = get_logprobs(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 1016, in get_logprobs
[rank0]:     ranks = _get_ranks(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 918, in _get_ranks
[rank0]:     return result.sum(1).add_(1)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.69 GiB. GPU 0 has a total capacity of 39.38 GiB of which 2.44 GiB is free. Including non-PyTorch memory, this process has 36.92 GiB memory in use. Of the allocated memory 36.30 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 28.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/bin/lm_eval", line 8, in <module>
[rank0]:     sys.exit(cli_evaluate())
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/__main__.py", line 375, in cli_evaluate
[rank0]:     results = evaluator.simple_evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 277, in simple_evaluate
[rank0]:     results = evaluate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 449, in evaluate
[rank0]:     resps = getattr(lm, reqtype)(cloned_reqs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/api/model.py", line 371, in loglikelihood
[rank0]:     return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 443, in _loglikelihood_tokens
[rank0]:     outputs = self._model_generate(requests=inputs, generate=False)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 274, in _model_generate
[rank0]:     outputs = self.model.generate(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/utils.py", line 1063, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 406, in generate
[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 942, in _run_engine
[rank0]:     step_outputs = self.llm_engine.step()
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 1454, in step
[rank0]:     outputs = self.model_executor.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 125, in execute_model
[rank0]:     output = self.driver_worker.execute_model(execute_model_req)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/worker_base.py", line 343, in execute_model
[rank0]:     output = self.model_runner.execute_model(
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 152, in _wrapper
[rank0]:     raise type(err)(
[rank0]: torch.OutOfMemoryError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241123-001502.pkl): CUDA out of memory. Tried to allocate 7.69 GiB. GPU 0 has a total capacity of 39.38 GiB of which 2.44 GiB is free. Including non-PyTorch memory, this process has 36.92 GiB memory in use. Of the allocated memory 36.30 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 28.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processed prompts:   0%|          | 0/40 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Running loglikelihood requests:   0%|          | 0/40 [00:00<?, ?it/s]
[rank0]:[W1123 00:15:03.529265438 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Sat Nov 23 00:15:05 UTC 2024
Sat Nov 23 00:15:26 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.25,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples --limit 10
nohup: ignoring input
2024-11-23:00:15:37,896 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-23:00:15:37,896 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-23:00:15:49,907 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-11-23:00:15:49,908 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-23:00:15:49,915 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-23:00:15:49,915 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.25, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
INFO 11-23 00:16:00 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 11-23 00:16:00 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-23 00:16:01 selector.py:135] Using Flash Attention backend.
INFO 11-23 00:16:02 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B...
INFO 11-23 00:16:02 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 11-23 00:16:03 weight_utils.py:288] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.87s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.87s/it]

INFO 11-23 00:16:05 model_runner.py:1077] Loading model weights took 2.3185 GB
INFO 11-23 00:16:05 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=2.82GiB peak_torch_memory=3.52GiB memory_usage_post_profile=2.84GiB non_torch_memory=0.51GiB kv_cache_size=5.81GiB gpu_memory_utilization=0.25
INFO 11-23 00:16:05 gpu_executor.py:113] # GPU blocks: 11900, # CPU blocks: 8192
INFO 11-23 00:16:05 gpu_executor.py:117] Maximum concurrency for 8192 tokens per request: 23.24x
INFO 11-23 00:16:09 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-23 00:16:09 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-23 00:16:20 model_runner.py:1518] Graph capturing finished in 12 secs, took 0.14 GiB
2024-11-23:00:16:25,201 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-23:00:16:25,214 WARNING  [task.py:325] [Task: meta_mmlu] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2024-11-23:00:16:26,873 INFO     [evaluator.py:261] Setting fewshot random generator seed to 42
2024-11-23:00:16:26,877 INFO     [task.py:411] Building contexts for meta_mmlu on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 44384.17it/s]
2024-11-23:00:16:26,891 INFO     [evaluator.py:438] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/40 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/40 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   2%|▎         | 1/40 [00:00<00:29,  1.33it/s, est. speed input: 741.07 toks/s, output: 1.33 toks/s][A
Processed prompts:  40%|████      | 16/40 [00:00<00:01, 23.55it/s, est. speed input: 9598.60 toks/s, output: 17.94 toks/s][AProcessed prompts: 100%|██████████| 40/40 [00:00<00:00, 44.39it/s, est. speed input: 22654.56 toks/s, output: 44.39 toks/s]
Running loglikelihood requests:   2%|▎         | 1/40 [00:00<00:35,  1.10it/s]Running loglikelihood requests: 100%|██████████| 40/40 [00:00<00:00, 43.08it/s]
2024-11-23:00:16:30,163 INFO     [evaluation_tracker.py:182] Saving results aggregated
2024-11-23:00:16:30,172 INFO     [evaluation_tracker.py:258] Saving per-sample results for: meta_mmlu
vllm (pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.25,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42), gen_kwargs: (None), limit: 10.0, num_fewshot: None, batch_size: auto
|    Tasks    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
| - meta_mmlu |      1|none  |     0|acc     |↑  |  0.3|±  |0.1528|
|             |       |none  |     0|acc_norm|↑  |  0.3|±  |0.1528|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |  0.3|±  |0.1528|
|             |       |none  |     0|acc_norm|↑  |  0.3|±  |0.1528|

|   Groups    |Version|Filter|n-shot| Metric |   |Value|   |Stderr|
|-------------|-------|------|-----:|--------|---|----:|---|-----:|
|meta_pretrain|N/A    |none  |     0|acc     |↑  |  0.3|±  |0.1528|
|             |       |none  |     0|acc_norm|↑  |  0.3|±  |0.1528|

[rank0]:[W1123 00:16:31.043248248 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
end: Sat Nov 23 00:16:33 UTC 2024
