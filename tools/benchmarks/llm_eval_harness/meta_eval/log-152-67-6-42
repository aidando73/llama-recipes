Fri Nov 22 22:23:30 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B --tasks mmlu_abstract_algebra --num_fewshot 5 --output_path eval_results --device cuda:0 --batch_size 8 --log_samples --limit 5
nohup: ignoring input
bash: line 1: lm_eval: command not found
end: Fri Nov 22 22:23:30 UTC 2024
Fri Nov 22 22:25:35 UTC 2024 - running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_pretrain --batch_size auto --output_path eval_results --include_path /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir --seed 42 --log_samples
nohup: ignoring input
2024-11-22:22:26:08,629 INFO     [__main__.py:272] Verbosity set to INFO
2024-11-22:22:26:08,629 INFO     [__main__.py:303] Including path: /home/ubuntu/1xa100-2/llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/work_dir
2024-11-22:22:26:20,708 INFO     [__main__.py:369] Selected Tasks: ['meta_pretrain']
2024-11-22:22:26:20,715 INFO     [evaluator.py:152] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42
2024-11-22:22:26:20,715 INFO     [evaluator.py:189] Initializing vllm model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'tensor_parallel_size': 1, 'dtype': 'auto', 'gpu_memory_utilization': 0.9, 'data_parallel_size': 1, 'max_model_len': 8192, 'add_bos_token': True, 'seed': 42}
Traceback (most recent call last):
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 406, in hf_raise_for_status
    response.raise_for_status()
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/sentence_bert_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/bin/lm_eval", line 8, in <module>
    sys.exit(cli_evaluate())
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/__main__.py", line 375, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/utils.py", line 395, in _wrapper
    return fn(*args, **kwargs)
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/evaluator.py", line 192, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/api/model.py", line 148, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/lm_eval/models/vllm_causallms.py", line 103, in __init__
    self.model = LLM(**self.model_args)
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/utils.py", line 1028, in inner
    return fn(*args, **kwargs)
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 210, in __init__
    self.llm_engine = self.engine_class.from_engine_args(
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 582, in from_engine_args
    engine_config = engine_args.create_engine_config()
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 959, in create_engine_config
    model_config = self.create_model_config()
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/engine/arg_utils.py", line 891, in create_model_config
    return ModelConfig(
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/config.py", line 214, in __init__
    self.encoder_config = self._get_encoder_config()
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/config.py", line 287, in _get_encoder_config
    return get_sentence_transformer_tokenizer_config(
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/transformers_utils/config.py", line 383, in get_sentence_transformer_tokenizer_config
    encoder_dict = get_hf_file_to_dict(config_name, model, revision, token)
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/transformers_utils/config.py", line 263, in get_hf_file_to_dict
    if file_or_path_exists(model=model,
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/vllm/transformers_utils/config.py", line 91, in file_or_path_exists
    return file_exists(model, config_name, revision=revision, token=token)
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/huggingface_hub/hf_api.py", line 2907, in file_exists
    get_hf_file_metadata(url, token=token)
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1296, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 277, in _request_wrapper
    response = _request_wrapper(
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 301, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/ubuntu/1xa100-2/llama-recipes/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 423, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-6741050d-40318fab27f6314b23a50327;dd6419ce-0f68-4501-97bd-f7b5f5e4a1c2)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/sentence_bert_config.json.
Access to model meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it. Please log in.
end: Fri Nov 22 22:26:23 UTC 2024
